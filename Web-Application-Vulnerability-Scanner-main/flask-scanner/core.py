import requests
from bs4 import BeautifulSoup
import re
import json
import time

class WebVulnerabilityScanner:
    def __init__(self, target_url, delay=1):
        self.target_url = target_url
        self.scanned_urls = set()
        self.vulnerabilities = []
        self.session = requests.Session()
        self.delay = delay # Delay between requests to be polite

        # Safe XSS payloads (Reflected XSS)
        self.xss_payloads = [
            "<script>alert(1)</script>",
            "<img src=x onerror=alert(1)>",
            "';alert(1)//"
        ]

        # Non-destructive SQLi payloads (Boolean-based, Error-based indicators)
        self.sqli_payloads = [
            "' OR '1'='1",
            "\" OR \"1\"=\"1",
            "1 OR 1=1",
            "' OR 1=1 --",
            "\" OR 1=1 --",
            "SLEEP(5)", # Time-based, careful with this on production
            "'", # Error-based
            "\"" # Error-based
        ]

        self.security_headers_to_check = [
            "Content-Security-Policy",
            "X-Frame-Options",
            "X-Content-Type-Options",
            "Strict-Transport-Security",
            "Referrer-Policy"
        ]

    def _make_request(self, url, method="GET", data=None):
        try:
            if method == "POST":
                response = self.session.post(url, data=data, timeout=10)
            else:
                response = self.session.get(url, timeout=10)
            time.sleep(self.delay)
            return response
        except requests.exceptions.RequestException as e:
            print(f"Error making request to {url}: {e}")
            return None

    def _extract_links(self, html_content, base_url):
        soup = BeautifulSoup(html_content, 'lxml')
        links = set()
        for a_tag in soup.find_all('a', href=True):
            href = a_tag['href']
            full_url = requests.compat.urljoin(base_url, href)
            if full_url.startswith(self.target_url) and full_url not in self.scanned_urls:
                links.add(full_url)
        return list(links)

    def _extract_forms(self, html_content, url):
        soup = BeautifulSoup(html_content, 'lxml')
        forms = []
        for form_tag in soup.find_all('form'):
            action = form_tag.get('action')
            method = form_tag.get('method', 'get').upper()
            form_url = requests.compat.urljoin(url, action if action else url)
            inputs = []
            for input_tag in form_tag.find_all(['input', 'textarea', 'select']):
                input_name = input_tag.get('name')
                input_type = input_tag.get('type', 'text')
                if input_name:
                    inputs.append({'name': input_name, 'type': input_type})
            forms.append({'url': form_url, 'method': method, 'inputs': inputs})
        return forms

    def _test_xss(self, url, forms):
        for payload in self.xss_payloads:
            # Test in URL parameters
            if "?" in url:
                base_url, params = url.split("?", 1)
                for param in params.split("&"):
                    key, value = param.split("=", 1)
                    test_url = f"{base_url}?{key}={payload}"
                    response = self._make_request(test_url)
                    if response and payload in response.text:
                        self.vulnerabilities.append({
                            'type': 'Reflected XSS',
                            'url': test_url,
                            'payload': payload,
                            'severity': 'High',
                            'evidence': response.text[max(0, response.text.find(payload)-50):min(len(response.text), response.text.find(payload)+50)]
                        })
                        print(f"[XSS Found] {test_url}")

            # Test in form fields
            for form in forms:
                form_data = {}
                is_xss_testable = False
                for input_field in form['inputs']:
                    if input_field['type'] in ['text', 'search', 'email', 'url', 'textarea']:
                        form_data[input_field['name']] = payload
                        is_xss_testable = True
                    else:
                        form_data[input_field['name']] = "test" # Fill other fields with dummy data

                if is_xss_testable:
                    response = self._make_request(form['url'], method=form['method'], data=form_data)
                    if response and payload in response.text:
                        self.vulnerabilities.append({
                            'type': 'Reflected XSS',
                            'url': form['url'],
                            'payload': payload,
                            'severity': 'High',
                            'evidence': response.text[max(0, response.text.find(payload)-50):min(len(response.text), response.text.find(payload)+50)],
                            'method': form['method'],
                            'form_data': form_data
                        })
                        print(f"[XSS Found] {form['url']} (Form)")


    def _test_sqli(self, url, forms):
        for payload in self.sqli_payloads:
            # Test in URL parameters
            if "?" in url:
                base_url, params = url.split("?", 1)
                for param in params.split("&"):
                    key, value = param.split("=", 1)
                    test_url = f"{base_url}?{key}={value}{payload}"
                    response = self._make_request(test_url)
                    if response:
                        # Simple check for common SQL error messages or boolean-based response changes
                        if "SQL syntax" in response.text or "mysql_fetch_array" in response.text or "ODBC" in response.text or "error in your SQL syntax" in response.text:
                             self.vulnerabilities.append({
                                'type': 'SQL Injection (Error-based)',
                                'url': test_url,
                                'payload': payload,
                                'severity': 'High',
                                'evidence': response.text[max(0, response.text.find("SQL syntax")-50):min(len(response.text), response.text.find("SQL syntax")+50)] if "SQL syntax" in response.text else response.text[:100]
                            })
                             print(f"[SQLi Found] {test_url}")
                        elif "SLEEP(5)" in payload and response.elapsed.total_seconds() > 4: # Time-based SQLi
                            self.vulnerabilities.append({
                                'type': 'SQL Injection (Time-based)',
                                'url': test_url,
                                'payload': payload,
                                'severity': 'High',
                                'evidence': f"Response time: {response.elapsed.total_seconds()}s"
                            })
                            print(f"[SQLi Found] {test_url}")
                        # Add more sophisticated boolean-based checks here if needed, comparing responses.

            # Test in form fields
            for form in forms:
                form_data = {}
                is_sqli_testable = False
                for input_field in form['inputs']:
                    if input_field['type'] in ['text', 'search', 'email', 'url', 'password', 'textarea']:
                        form_data[input_field['name']] = f"test{payload}"
                        is_sqli_testable = True
                    else:
                        form_data[input_field['name']] = "test"

                if is_sqli_testable:
                    response = self._make_request(form['url'], method=form['method'], data=form_data)
                    if response:
                        if "SQL syntax" in response.text or "mysql_fetch_array" in response.text or "ODBC" in response.text or "error in your SQL syntax" in response.text:
                             self.vulnerabilities.append({
                                'type': 'SQL Injection (Error-based)',
                                'url': form['url'],
                                'payload': payload,
                                'severity': 'High',
                                'evidence': response.text[max(0, response.text.find("SQL syntax")-50):min(len(response.text), response.text.find("SQL syntax")+50)] if "SQL syntax" in response.text else response.text[:100],
                                'method': form['method'],
                                'form_data': form_data
                            })
                             print(f"[SQLi Found] {form['url']} (Form)")
                        elif "SLEEP(5)" in payload and response.elapsed.total_seconds() > 4:
                            self.vulnerabilities.append({
                                'type': 'SQL Injection (Time-based)',
                                'url': form['url'],
                                'payload': payload,
                                'severity': 'High',
                                'evidence': f"Response time: {response.elapsed.total_seconds()}s",
                                'method': form['method'],
                                'form_data': form_data
                            })
                            print(f"[SQLi Found] {form['url']} (Form)")


    def _check_security_headers(self, url, headers):
        missing_headers = []
        for header_name in self.security_headers_to_check:
            if header_name not in headers:
                missing_headers.append(header_name)
        if missing_headers:
            self.vulnerabilities.append({
                'type': 'Missing Security Headers',
                'url': url,
                'payload': f"Missing headers: {', '.join(missing_headers)}",
                'severity': 'Medium',
                'evidence': f"Headers received: {headers}"
            })
            print(f"[Missing Headers] {url}: {', '.join(missing_headers)}")

    def crawl_and_scan(self, max_links=50):
        self.scanned_urls.add(self.target_url)
        urls_to_scan = [self.target_url]
        crawled_count = 0

        while urls_to_scan and crawled_count < max_links:
            current_url = urls_to_scan.pop(0)
            print(f"Scanning: {current_url}")
            response = self._make_request(current_url)
            crawled_count += 1

            if response and response.status_code == 200:
                # Check security headers
                self._check_security_headers(current_url, response.headers)

                # Extract forms
                forms = self._extract_forms(response.text, current_url)

                # Test for XSS
                self._test_xss(current_url, forms)

                # Test for SQLi
                self._test_sqli(current_url, forms)

                # Extract and add new links
                new_links = self._extract_links(response.text, current_url)
                for link in new_links:
                    if link not in self.scanned_urls:
                        urls_to_scan.append(link)
                        self.scanned_urls.add(link)
                        if len(self.scanned_urls) >= max_links:
                            break # Limit crawling to max_links

        return self.vulnerabilities

    def generate_report(self, filename="vulnerability_report.json"):
        with open(filename, 'w') as f:
            json.dump(self.vulnerabilities, f, indent=4)
        print(f"Report generated: {filename}")
        return self.vulnerabilities